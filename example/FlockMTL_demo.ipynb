{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlockMTL Extension Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You work for an e-commerce company that sells consumer electronics. You have a set of customer reviews, and your goal is to:\n",
    "\n",
    "#### 1. Analyze the sentiment of each review, and get the top K similar reviews\n",
    "#### 2. Identify high-impact reviews that require urgent attention.\n",
    "#### 3. Extract recurring themes or topics to understand common issues and areas for product improvement.\n",
    "#### 4. Track customer satisfaction trends over time.\n",
    "\n",
    "### Let us see how to build SQL queries that use `FlockMTL` to achieve all of this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "#### Step 1: Load your OpenAI API key into the current env\n",
    "* We are loading it via the .env file using dotenv python pkg.\n",
    "* Create a file called .env in the current working directory, with the following text:\n",
    "* `export OPENAI_API_KEY='<your OPENAI API KEY'`\n",
    "* Replace the placeholder with your API key.\n",
    "\n",
    "\n",
    "You can also set key as an environment variable, using `export` or `set`, depending upon your terminal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Install and import DuckDB\n",
    "\n",
    "We are using DuckDB version 1.1.1 for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install duckdb==1.1.1\n",
    "import duckdb\n",
    "print(duckdb.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything goes well, you should see the version being printed, its 1.1.1 for our example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Create a database for our use\n",
    "\n",
    "1. We create a new persistent database connection for our example called `mydb.db`.\\\n",
    "2. We also pass a config dict `config={'allow_unsigned_extensions' : 'true'}` to allow us to load the extension.\n",
    "3. We also load a CSV file called `product_reviews.csv` into a table called `product_reviews`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection to an in-memory database\n",
    "# You can also provide a persistent database by replacing `:memory` with your database name\n",
    "con = duckdb.connect(':memory:', config={'allow_unsigned_extensions' : 'true'})\n",
    "\n",
    "csv_path = 'product_reviews.csv' #add your own path\n",
    "con.execute(f\"CREATE TABLE product_reviews AS SELECT * FROM read_csv_auto('{csv_path}')\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Load our extension\n",
    "\n",
    "\n",
    "We use the DuckDB `INSTALL` and `LOAD` commands to load our extension\\\n",
    "Set the `path` variable to the location where the extension binary is downloaded, and then execute the following\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = '<FlockMTL extension binary path>' # replace your path here\n",
    "con.execute(f\"LOAD '{path}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The FlockMTL extension is now loaded into DuckDB\n",
    "* Out of the box we provide support for two different LLMs - `gpt-4o` and `gpt-4o-mini`, the default being `gpt-4o-mini`\n",
    "* You can list the available models by runnning the  query `GET MODELS;`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"GET MODELS;\").fetchall()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We also provide a default `hello-world` prompt\n",
    "* You can list the available running by runnning the query `GET PROMPTS;` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"GET PROMPTS;\").fetchall()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us run the hello-world prompt\n",
    "\n",
    "Execute the following query to run the `hello-world` prompt. We use `default` as our LLM, which is `gpt-4o-mini`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT llm_complete('hello-world', 'default');\n",
    "\"\"\"\n",
    "\n",
    "result = con.execute(query).fetchall()\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And that's it ! Our extension is loaded, now we can use LLM capabilities within SQL!\n",
    "\n",
    "Using models, prompts, and llm scalar functions directly with SQL, we enable semantic analysis into our system\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's go through the tasks one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing Sentiment Analysis on Product Reviews:\n",
    "* The first step is to analyze the sentiment of each review. We can use LLMs to generate a detailed sentiment analysis by combining the review text with the star rating. We also extract top 5 similar reviews for each review\n",
    "\n",
    "* There are two steps in this task - first defining a `sentiment-analysis` prompt, and second, using the prompt to perform sentiment analysis on the table\n",
    "\n",
    "* We use `llm_complete_json`, a function that calls the `default` model to perform sentiment analysis on each review.\n",
    "`default` is an alias for `gpt-4o-mini` \n",
    "\n",
    "* The model evaluates the sentiment based on both the review text and the star rating.\n",
    "\n",
    "* This step gives a sentiment score or label (e.g., positive, negative, neutral) for each review, providing a solid foundation for deeper analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First we create a new sentiment analysis prompt\n",
    "sentiment_analysis_prompt = \"\"\"\n",
    "Analyze the sentiment of the following product review. Consider both the review text and the star rating. Provide a brief sentiment label (positive, negative, or neutral) and a short explanation for your decision.\n",
    "\n",
    "Review: {review}\n",
    "Star Rating: {rating}\n",
    "\n",
    "Output your response in the following JSON format:\n",
    "{\n",
    "    \"sentiment\": \"positive/negative/neutral\",\n",
    "    \"explanation\": \"Brief explanation of the sentiment analysis\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Use an f-string to insert the prompt directly into the query\n",
    "sentiment_analysis_prompt_query = f\"\"\"\n",
    "    CREATE PROMPT ('sentiment-analysis', '{sentiment_analysis_prompt}');\n",
    "\"\"\"\n",
    "\n",
    "con.execute (sentiment_analysis_prompt_query)\n",
    "\n",
    "\n",
    "\n",
    "# Now we use the new prompt for analytical analysis\n",
    "query = \"\"\"\n",
    "CREATE TABLE sentiment_analysis AS\n",
    "WITH sentiment_analysis AS (\n",
    "    SELECT \n",
    "        ProductID as product_id, \n",
    "        ID AS review_id,\n",
    "        Review AS review_text, \n",
    "        Rating AS star_rating, \n",
    "        llm_complete_json('sentiment-analysis', 'default', {'review': review_text, 'rating': star_rating}) AS sentiment_json\n",
    "    FROM \n",
    "        product_reviews\n",
    ")\n",
    "SELECT * \n",
    "FROM sentiment_analysis;\n",
    "\"\"\"\n",
    "\n",
    "# Run the query and fetch results\n",
    "results = con.execute(query).fetchall()\n",
    "\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets take a look at the newly created sentiment_analysis table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select * from sentiment_analysis;\"\n",
    "results = con.execute(query).fetchall()\n",
    "\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sentiment-analysis` prompt is now available to be reused directly for later tasks as well, no need for redefinition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting top k similar reviews\n",
    "* We want the top k similar reviews for each review\n",
    "* We define a new prompt `get-embedding` to generate embeddings for each review, using the `llm_complete` function\n",
    "* We use the embeddings to perform `cosine similarity` for each pair of reviews\n",
    "* We then return `top-k` similar review to each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the get-embedding prompt\n",
    "get_embedding_prompt = \"\"\"\n",
    "    Generate an embedding vector for the following product review. \n",
    "    This embedding should capture the key semantic meaning of the review text.\n",
    "\n",
    "    Review: {review}\n",
    "\n",
    "    Output the embedding as FLOAT ARRAY datatype of size 3, do not return as JSON and VARCHAR.\n",
    "\"\"\"\n",
    "\n",
    "# Create the SQL query to register the get-embedding prompt\n",
    "get_embedding_prompt_query = f\"\"\"\n",
    "    CREATE PROMPT ('get-embedding', '{get_embedding_prompt}');\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query to create the prompt\n",
    "con.execute(get_embedding_prompt_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"\"\"\n",
    "WITH review_embeddings AS (\n",
    "    SELECT \n",
    "        ID AS review_id,\n",
    "        Review AS review_text,\n",
    "        llm_complete('get-embedding', 'default', {'review': review_text}) AS embedding\n",
    "    FROM product_reviews\n",
    "),\n",
    "review_similarities AS (\n",
    "    SELECT\n",
    "        r.review_id AS query_review_id,\n",
    "        e.review_id AS similar_review_id,\n",
    "        1 - (SQRT(SUM(\n",
    "            POW(CAST(SPLIT_PART(SPLIT_PART(r.embedding, ',', 1), '[', 2) AS FLOAT) - CAST(SPLIT_PART(SPLIT_PART(e.embedding, ',', 1), '[', 2) AS FLOAT), 2) +\n",
    "            POW(CAST(SPLIT_PART(r.embedding, ',', 2) AS FLOAT) - CAST(SPLIT_PART(e.embedding, ',', 2) AS FLOAT), 2) +\n",
    "            POW(CAST(SPLIT_PART(SPLIT_PART(r.embedding, ',', 3), ']', 1) AS FLOAT) - CAST(SPLIT_PART(SPLIT_PART(e.embedding, ',', 3), ']', 1) AS FLOAT), 2)\n",
    "        ) / 3)) AS similarity\n",
    "    FROM review_embeddings r\n",
    "    JOIN review_embeddings e ON r.review_id <> e.review_id\n",
    "    GROUP BY r.review_id, e.review_id\n",
    ")\n",
    "\n",
    "SELECT * \n",
    "FROM review_similarities;\n",
    "\n",
    "\"\"\"\n",
    "results = con.execute(query2).fetchall()\n",
    "\n",
    "for row in results:\n",
    "    print (row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the `top 5 similar` reviews for each review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering for High-Impact Reviews\n",
    "Not all reviews are equally important. Some are more detailed, more critical, or offer particularly valuable feedback. In this next step, we filter out the \"high-impact\" reviews—those that deserve immediate attention.\n",
    "\n",
    "This query introduces `llm_filter`, which uses the current model to determine whether a review is \"high-impact\" based on the sentiment, rating, and length of the review. High-impact reviews could include:\n",
    "\n",
    "Negative reviews that are detailed and provide insights into product issues.\n",
    "Positive reviews that highlight key features or advantages (useful for marketing).\n",
    "By focusing on these high-impact reviews, you can address critical feedback and leverage valuable testimonials effectively.\n",
    "\n",
    "We define a new prompt `is-high-impact-review` which classifies whether a review is \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the is-high-impact-review prompt\n",
    "is_high_impact_review_prompt = \"\"\"\n",
    "Determine if the given review is a high-impact review that provides valuable insights. Consider the following factors:\n",
    "\n",
    "1. Sentiment: {sentiment}\n",
    "2. Star Rating: {rating}\n",
    "3. Review Length: {review_length}\n",
    "\n",
    "A high-impact review typically has:\n",
    "\n",
    "- A strong sentiment (very positive or very negative)\n",
    "- An extreme rating (1-2 or 4-5 stars)\n",
    "- Sufficient length to provide detailed feedback (usually more than 50 words)\n",
    "\n",
    "Output your decision as a boolean true or false.\n",
    "\"\"\"\n",
    "\n",
    "# Create the SQL query to register the is-high-impact-review prompt\n",
    "is_high_impact_review_prompt_query = f\"\"\"\n",
    "    CREATE PROMPT ('is-high-impact-review', '{is_high_impact_review_prompt}');\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query to create the prompt\n",
    "con.execute(is_high_impact_review_prompt_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE filtered_reviews AS\n",
    "WITH filtered_reviews AS (\n",
    "    SELECT \n",
    "        * \n",
    "    FROM \n",
    "        sentiment_analysis \n",
    "    WHERE \n",
    "        llm_filter('is-high-impact-review', 'gpt-4o', {\n",
    "            'sentiment': sentiment_json, \n",
    "            'rating': star_rating, \n",
    "            'review_length': LENGTH(review_text)\n",
    "        })\n",
    ")\n",
    "SELECT * \n",
    "FROM filtered_reviews;\n",
    "\"\"\"\n",
    "\n",
    "# Run the query and fetch results\n",
    "results = con.execute(query).fetchall()\n",
    "\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Key Themes from Reviews\n",
    "\n",
    "Once you’ve identified high-impact reviews, the next step is understanding why customers leave them.\n",
    "This query helps identify recurring themes such as complaints about battery life, praises for design, or issues with customer service.\n",
    "This query extracts common themes using LLM's ability to understand and categorize text. You’ll get insights like:\n",
    "\n",
    "Common Complaints: \"The battery drains too quickly.\"\n",
    "Positive Feedback: \"The screen resolution is excellent.\"\n",
    "\n",
    "We first define our own prompt called `extract-themes`, and then use it to extract the themes from `filtered_reviews`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the extract themes prompt\n",
    "extract_themes_prompt = \"\"\"\n",
    "    Analyze the following sentiment analysis JSON. Identify and extract key themes or topics discussed in the product review. Output the themes in a JSON array format.\n",
    "    \n",
    "    Sentiment Analysis JSON: {{sentiment_json}}\n",
    "    \n",
    "    Output your response in the following JSON format:\n",
    "    {{\n",
    "        \"themes\": [\"theme1\", \"theme2\", \"theme3\"]\n",
    "    }}\n",
    "\"\"\"\n",
    "       \n",
    "# Create the SQL query to register the prompt\n",
    "extract_themes_prompt_query = f\"\"\"\n",
    "    CREATE PROMPT ('extract-themes' , '{extract_themes_prompt}');\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query to create the prompt\n",
    "results = con.execute(extract_themes_prompt_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE themes_extracted AS\n",
    "WITH themes_extracted AS (\n",
    "    SELECT \n",
    "        product_id, \n",
    "        review_id, \n",
    "        review_text,\n",
    "        star_rating,\n",
    "        llm_complete_json('extract-themes', 'gpt-4o', {'sentiment_json': sentiment_json}) AS themes\n",
    "    FROM \n",
    "        filtered_reviews\n",
    ")\n",
    "SELECT * \n",
    "FROM themes_extracted;\n",
    "\"\"\"\n",
    "# Run the query and fetch results\n",
    "results = con.execute(query).fetchall()\n",
    "\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking Customer Sentiment Over Time\n",
    "Customer feedback changes as new product updates are rolled out or customer expectations evolve. To track customer satisfaction over time, you can analyze how the average sentiment and star ratings change month by month.\n",
    "\n",
    "We use 2 different prompts here `sentiment_analysis_prompt` and `extract_sentiment_score_prompt_numeric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the extract sentiment score prompt\n",
    "extract_sentiment_score_prompt_numeric = \"\"\"\n",
    "Analyze the following sentiment analysis JSON and extract the sentiment score. \n",
    "Output the sentiment score as a numeric value.\n",
    "\n",
    "Sentiment Analysis JSON: {sentiment_json}\n",
    "\n",
    "Output your response as a number {sentiment_score}\n",
    "\"\"\"\n",
    "\n",
    "# Define the SQL query to create the prompt\n",
    "query = f\"\"\"\n",
    "CREATE PROMPT ('extract-sentiment-score', '{extract_sentiment_score_prompt_numeric}');\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query to create the prompt\n",
    "con.execute(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SQL query\n",
    "query = \"\"\"\n",
    "WITH sentiment_analysis_dated AS (\n",
    "    SELECT \n",
    "        ProductID as product_id, \n",
    "        ID AS review_id,\n",
    "        Review AS review_text, \n",
    "        Rating AS star_rating, \n",
    "        Date as review_date,\n",
    "        llm_complete_json('sentiment-analysis', 'gpt-4o', {'review': review_text, 'rating': star_rating}) AS sentiment_json\n",
    "    FROM \n",
    "        product_reviews\n",
    ")\n",
    "SELECT \n",
    "    product_id, \n",
    "    EXTRACT(YEAR FROM review_date) AS year, \n",
    "    EXTRACT(MONTH FROM review_date) AS month, \n",
    "    AVG(star_rating) AS avg_star_rating, \n",
    "    AVG(\n",
    "        CAST(\n",
    "            (llm_complete_json('extract-sentiment-score', 'gpt-4o', {'sentiment_json': sentiment_json}))->>'sentiment_score' AS DOUBLE\n",
    "        )\n",
    "    )\n",
    "     AS avg_sentiment_score\n",
    "FROM \n",
    "    sentiment_analysis_dated\n",
    "GROUP BY \n",
    "    product_id, \n",
    "    EXTRACT(YEAR FROM review_date), \n",
    "    EXTRACT(MONTH FROM review_date)\n",
    "ORDER BY \n",
    "    year DESC, month DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and fetch the results\n",
    "results = con.execute(query).fetchall()\n",
    "\n",
    "# Print the results\n",
    "for row in results:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And that is it, we have successfully created new prompts, used existing prompts and manipulated the standard table to gain analytical insights in our data, using the power of LLMs!\n",
    "\n",
    "We finally close the DuckDB connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the DuckDB connection\n",
    "con.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
